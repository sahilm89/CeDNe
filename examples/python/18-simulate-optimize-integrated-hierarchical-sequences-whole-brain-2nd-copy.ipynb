{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from cedne import simulator\n",
    "from cedne import optimizer\n",
    "from cedne import utils\n",
    "from cedne import cedne\n",
    "from collections import Counter\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import json"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train parameters layer by layer, not all at once. That is the benefit of using hierarchical sequences over time!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not os.path.isdir(utils.OUTPUT_DIR):\n",
    "    os.makedirs(utils.OUTPUT_DIR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ntype = ['sensory', 'interneuron', 'motorneuron']\n",
    "facecolors = ['#FF6F61', '#FFD700', '#4682B4']\n",
    "ntype_pairs = set([tuple(sorted([nt1, nt2])) for nt1 in ntype for nt2 in ntype])\n",
    "colors= plt.cm.magma(np.linspace(0,1,len(ntype_pairs)))\n",
    "type_color_dict = {p:color for (p,color) in zip(ntype_pairs, colors)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w = utils.makeWorm(chem_only=True)\n",
    "nn_chem = w.networks[\"Neutral\"]\n",
    "\n",
    "# w_both = utils.makeWorm()\n",
    "# nn_both = w_both.networks[\"Neutral\"] \n",
    "\n",
    "# w_gapjn = utils.makeWorm(gapjn_only=True)\n",
    "# nn_gapjn = w.networks[\"Neutral\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "triad_motifs = utils.return_triads()\n",
    "motif = triad_motifs['030T']\n",
    "motif = utils.nx.relabel_nodes(motif, {1:1, 2:3, 3:2})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hseq = utils.make_hypermotifs(motif, 3, [(3,1)])\n",
    "hseq = utils.nx.relabel_nodes(hseq, {'1.3-2.1':'2.1', '2.3-3.1':'3.1'})\n",
    "hseq = utils.nx.convert_node_labels_to_integers(hseq, first_label=1, ordering='sorted', label_attribute='nodename')\n",
    "all_ffgs = nn_chem.search_motifs(hseq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "command_interneurons = ['AVAL', 'AVAR', 'AVBL', 'AVBR', 'AVDL', 'AVDR', 'AVEL', 'AVER']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "edgelist = []\n",
    "input_neurons = []\n",
    "\n",
    "mot_edgelabels = {node:[] for node in hseq}\n",
    "neuron_layers = {node:[] for node in hseq}\n",
    "for ffg in all_ffgs:\n",
    "    nodelist = {node:None for node in hseq}\n",
    "    for med, ned in ffg.items():\n",
    "        for m,n in zip(med, ned):\n",
    "            nodelist[m] = n.name\n",
    "    # if nodelist[5] in command_interneurons:\n",
    "        edgelist+= [(e[0], e[1], 0) for e in ffg.values() if not (e[0], e[1], 0) in edgelist]\n",
    "        # input_neurons.append(nodelist[1])\n",
    "        if nn_chem.neurons[nodelist[1]].type == 'sensory':\n",
    "            input_neurons.append(nodelist[1])\n",
    "        for m in nodelist:\n",
    "            neuron_layers[m].append(nodelist[m])\n",
    "input_neurons = list(sorted(set(input_neurons)))\n",
    "neuron_layers = {layer: list(set(neuron_layers[layer])) for layer in neuron_layers}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nn_chem_sub_pre = nn_chem.subnetwork(connections=edgelist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(conn_neu_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preserveNeuron(neuron, imaging_data, nn, alpha=0.33, beta=0.66):\n",
    "    # Condition 1: The neuron must have imaging data\n",
    "    if neuron not in imaging_data:\n",
    "        return False\n",
    "\n",
    "    # Condition 2: Upstream Support\n",
    "    inputs = set([m1.name for m1,m2,i in nn.neurons[neuron].incoming()])\n",
    "    if len(inputs) > 0:\n",
    "        preserved_inputs = [n for n in inputs if n in imaging_data]\n",
    "        if len(preserved_inputs) / len(inputs) >= alpha:\n",
    "            return True\n",
    "\n",
    "    # Condition 3: Downstream Support\n",
    "    outputs = set([m2.name for m1,m2,i in nn.neurons[n].outgoing()])\n",
    "    if len(outputs)>0:\n",
    "        downstream_loss = sum(\n",
    "                (len([m1.name for m1,m2,i in nn.neurons[o].incoming() if m1.name in imaging_data] )-1)/ len(nn.neurons[o].incoming())\n",
    "                for o in outputs if len(nn.neurons[o].incoming())\n",
    "            ) / len(outputs)  # Take the mean instead of sum\n",
    "        if downstream_loss < beta:\n",
    "            return True\n",
    "    return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "counter = 0\n",
    "alphas = np.linspace(0,1,11)\n",
    "betas = np.linspace(0,1,11)\n",
    "\n",
    "m_neur = np.zeros((11,11))\n",
    "for i,alpha in enumerate(alphas):\n",
    "    for j,beta in enumerate(betas):\n",
    "        optim_neurs = {}\n",
    "        for n in nn_chem.neurons:\n",
    "            for database,p in jsons.items():\n",
    "                sortedKeys = sorted ([int(x) for x in (p['labeled'].keys())])\n",
    "                labelledNeurons = {p['labeled'][str(x)]['label']:x for x in sortedKeys if not '?' in p['labeled'][str(x)]['label']}\n",
    "\n",
    "                if not database in optim_neurs.keys():\n",
    "                    optim_neurs[database] = []\n",
    "                if preserveNeuron(n, labelledNeurons, nn_chem, alpha, beta):\n",
    "                    optim_neurs[database].append(n)\n",
    "        mean_neurs = np.mean([len(optim_neurs[database]) for database in optim_neurs])\n",
    "        m_neur[i,j] = mean_neurs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f, ax = plt.subplots()\n",
    "cbar = ax.pcolor(m_neur)\n",
    "ax.set_xticks(np.arange(len(alphas)) + 0.5, [f\"{s:0.1f}\" for s in alphas], rotation=45)\n",
    "ax.set_yticks(np.arange(len(betas)) + 0.5, [f\"{s:0.1f}\" for s in betas])\n",
    "f.colorbar(cbar)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preserveNeuron2(neuron, imaging_data, nn, alpha=0.33):\n",
    "    # Condition 1: The neuron must have imaging data\n",
    "    # Condition 2: Upstream Support\n",
    "    inputs, weights = zip(*[(m1.name, nn.connections[(m1,m2,i)].weight) for m1,m2,i in nn.neurons[neuron].incoming()])\n",
    "    if len(inputs) > 0:\n",
    "\n",
    "        preserved_inputs = [w for n,w in zip(inputs,weights) if n in imaging_data]\n",
    "        if sum(preserved_inputs) / sum(weights) >= alpha:\n",
    "            return True\n",
    "\n",
    "        # if sum(preserved_inputs) >= 10:\n",
    "        #     return True\n",
    "\n",
    "    # Condition 3: Downstream Support\n",
    "    # outputs = set([m2.name for m1,m2,i in nn.neurons[n].outgoing()])\n",
    "    # if len(outputs)>0:\n",
    "    #     downstream_loss = sum(\n",
    "    #             (len([m1.name for m1,m2,i in nn.neurons[o].incoming() if m1.name in imaging_data] )-1)/ len(nn.neurons[o].incoming())\n",
    "    #             for o in outputs if len(nn.neurons[o].incoming())\n",
    "    #         ) / len(outputs)  # Take the mean instead of sum\n",
    "    #     if downstream_loss < beta:\n",
    "    #         return True\n",
    "    return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "alpha= 0.33\n",
    "nonoptim_neurs = {}\n",
    "for database,p in jsons.items():\n",
    "    sortedKeys = sorted ([int(x) for x in (p['labeled'].keys())])\n",
    "    labelledNeurons = {p['labeled'][str(x)]['label']:x for x in sortedKeys if not '?' in p['labeled'][str(x)]['label']}\n",
    "    # nn_sub = nn_chem.subnetwork(neuron_names=labelledNeurons)\n",
    "    for n in labelledNeurons:\n",
    "        if not database in nonoptim_neurs.keys():\n",
    "            nonoptim_neurs[database] = []\n",
    "\n",
    "        if nn_chem.neurons[n].type not in ['sensory']:\n",
    "            if not preserveNeuron2(n, labelledNeurons, nn_chem, alpha):\n",
    "                nonoptim_neurs[database].append(n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for d in sorted(nonoptim_neurs):\n",
    "    print(d, len(nonoptim_neurs[d]))\n",
    "    for n in nonoptim_neurs[d]:\n",
    "        print(n, nn_chem.neurons[n].type)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(nn_chem.neurons['AUAR'].category)\n",
    "for e in nn_chem.neurons['AUAR'].outgoing():\n",
    "    print(e[1].name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "categories = set([nn_chem.neurons[n].category for n in nn_chem.neurons])\n",
    "# for category in categories:\n",
    "for database,p in jsons.items():\n",
    "    sortedKeys = sorted ([int(x) for x in (p['labeled'].keys())])\n",
    "    labelledNeurons = {p['labeled'][str(x)]['label']:x for x in sortedKeys if not '?' in p['labeled'][str(x)]['label']}\n",
    "    print([(n, nn_chem.neurons[n].category) for n in labelledNeurons if nn_chem.neurons[n].category in [\"sublateral motor neuron\", \"head motor neuron\"]])\n",
    "    print(len([n for n in labelledNeurons if nn_chem.neurons[n].category in [\"sublateral motor neuron\", \"head motor neuron\"]])/ len([n for n in nn_chem.neurons if nn_chem.neurons[n].category in [\"sublateral motor neuron\", \"head motor neuron\"]]) )\n",
    "    # print(Counter([nn_chem.neurons[n].category for n in labelledNeurons if nn_chem.neurons[n].type == 'motorneuron']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "counter = 0\n",
    "optimizable_neurs = {}\n",
    "for n in nn_chem.neurons:\n",
    "    # conn_neu_names = set([m2.name for m1,m2,id in nn_chem.neurons[n].outgoing()]) | set([m1.name for m1,m2,id in nn_chem.neurons[n].incoming()])\n",
    "    conn_neu_names = set([m1.name for m1,m2,id in nn_chem.neurons[n].incoming()])\n",
    "    fracs = []\n",
    "    pres = []\n",
    "    for database,p in jsons.items():\n",
    "        if not database in optimizable_neurs.keys():\n",
    "            optimizable_neurs[database] = []\n",
    "        sortedKeys = sorted ([int(x) for x in (p['labeled'].keys())])\n",
    "        labelledNeurons = {p['labeled'][str(x)]['label']:x for x in sortedKeys if not '?' in p['labeled'][str(x)]['label']}\n",
    "        if len(conn_neu_names):\n",
    "            frac = len(set(labelledNeurons.keys()) & conn_neu_names)/len(conn_neu_names)\n",
    "            if n in labelledNeurons.keys() and frac>0.33:\n",
    "                fracs.append(frac)\n",
    "                pres.append(n in set(labelledNeurons.keys()))\n",
    "                optimizable_neurs[database].append(n)\n",
    "    if len(fracs)>0:\n",
    "        counter+=1\n",
    "        print(counter, n, nn_chem.neurons[n].type, list(zip(pres,fracs)))\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(optimizable_neurs['Atanas et al (2023) 2022-06-14-01.json'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for n in nn_chem.neurons:\n",
    "    nn.neurons[n].get_neighbours()\n",
    "    for database in jsons.keys():\n",
    "        jsons[database]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def optimizable_neurons(common_set, nn_chem_sub):\n",
    "    neurons_for_optim = []\n",
    "    for n1 in nn_chem_sub.neurons:\n",
    "        if nn_chem_sub.neurons[n1].type in ['sensory', 'interneuron', 'motorneuron']:\n",
    "            # print([key[0].name for key in nn_chem_sub.neurons[n1].incoming().keys()])\n",
    "            optimizable = n1 in common_set# or (any([key[0].name in common_set for key in nn_chem_sub.neurons[n1].incoming().keys()]) and any([key[1].name in common_set for key in nn_chem_sub.neurons[n1].outgoing().keys()]))\n",
    "            if optimizable:\n",
    "                neurons_for_optim.append(n1)\n",
    "    return neurons_for_optim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "jsons = {}\n",
    "for js in os.listdir('/Users/sahilmoza/Documents/Postdoc/Yun Zhang/data/SteveFlavell-NeuroPAL-Cell/Control/'):\n",
    "    with open (\"/Users/sahilmoza/Documents/Postdoc/Yun Zhang/data/SteveFlavell-NeuroPAL-Cell/Control/{}\".format(js), 'r') as f:\n",
    "        jsons['Atanas et al (2023) ' +  js] = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "measuredNeurons = {}\n",
    "neuron_labels = []\n",
    "optim_neurs = {js:[] for js in jsons.keys()}\n",
    "for js, p in jsons.items():\n",
    "    sortedKeys = sorted ([int(x) for x in (p['labeled'].keys())])\n",
    "    labelledNeurons = {p['labeled'][str(x)]['label']:x for x in sortedKeys if not '?' in p['labeled'][str(x)]['label']} # Removing unsure hits\n",
    "    measuredNeurons[js] = {m:i for i,m in enumerate(set(labelledNeurons))}\n",
    "    \n",
    "    nlabs = list(measuredNeurons[js].keys())\n",
    "    neuron_labels+=nlabs\n",
    "    common_set = set(nn_chem_sub_pre.neurons).intersection(set(nlabs))\n",
    "    frac_common = len(common_set)/len(nn_chem_sub_pre.neurons)\n",
    "    neurons_for_optim = optimizable_neurons(common_set, nn_chem_sub_pre)\n",
    "    optim_neurs[js] = neurons_for_optim\n",
    "    # nn_chem_sub = nn_chem_sub_pre.subnetwork(neuron_names=neurons_for_optim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# measuredNeurons = {}\n",
    "# neuron_labels = []\n",
    "# for js, p in jsons.items():\n",
    "#     sortedKeys = sorted ([int(x) for x in (p['labeled'].keys())])\n",
    "#     labelledNeurons = {p['labeled'][str(x)]['label']:x for x in sortedKeys if not '?' in p['labeled'][str(x)]['label']} # Removing unsure hits\n",
    "#     measuredNeurons[js] = {m:i for i,m in enumerate(set(labelledNeurons))}\n",
    "#     neuron_labels+=measuredNeurons[js].keys()\n",
    "# neuron_labels = sorted(set(neuron_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp_arr= []\n",
    "database = list(jsons.keys())[0]\n",
    "for neuron in measuredNeurons[database].keys(): \n",
    "    temp_arr.append(jsons[database]['trace_array'][measuredNeurons[database][neuron]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp_arr = np.array(temp_arr)\n",
    "vm = np.max(np.abs(temp_arr))/5\n",
    "plt.pcolormesh(temp_arr[8:18, :], cmap='PuOr', vmax=vm, vmin=-vm)\n",
    "plt.axis(False)\n",
    "plt.savefig(\"ca-act_schematic.svg\", transparent=True)\n",
    "plt.show()\n",
    "plt.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_trials = 15\n",
    "window_length = 100\n",
    "window_step = 50\n",
    "best_models = {}\n",
    "best_loss = {}\n",
    "best_params = {}\n",
    "for datab_in, database in enumerate(jsons.keys()):\n",
    "    ## Subnetwork and optimize\n",
    "    best_models[database] = {}\n",
    "    best_loss[database] = {}\n",
    "    best_params[database] = {}\n",
    "    #     nn_chem_sub = nn_chem.subnetwork(neurons=optim_neurs[database])\n",
    "    nn_chem_sub = nn_chem.subnetwork(connections=edgelist)\n",
    "\n",
    "    ## Parameter Setup\n",
    "\n",
    "    tconstants = [1] *len(nn_chem_sub.nodes)\n",
    "    input_nodes = [nn_chem_sub.neurons[n] for n in input_neurons]\n",
    "\n",
    "    weights = {e:1 for e in nn_chem_sub.edges}\n",
    "    gains = {node:1.0 for node in nn_chem_sub.nodes}\n",
    "    baselines = {node:0. for node in nn_chem_sub.nodes}\n",
    "    time_constants = {n:t for n,t in zip(nn_chem_sub.nodes, tconstants)}\n",
    "    num_timepoints = len(jsons[database]['trace_array'][measuredNeurons[database][list(measuredNeurons[database].keys())[0]]])\n",
    "    time_points_all = np.arange(num_timepoints)#jsons[database]['max_t'])\n",
    "\n",
    "    gain_base = {n.name:1 for n in nn_chem_sub.nodes} # units = _\n",
    "    tconst_base = {n.name:1 for n in nn_chem_sub.nodes} # units = time_points ^-1\n",
    "    base_base = {n.name:1 for n in nn_chem_sub.nodes}\n",
    "    wts_base = {(n[0].name, n[1].name, n[2]):1 for n in nn_chem_sub.edges}\n",
    "\n",
    "    gain_lims = np.array([-1,1])\n",
    "    tconst_lims = np.array([2,50])\n",
    "    base_lim = np.array([-2,2])\n",
    "    wts_lim = np.array([-2,2])\n",
    "\n",
    "    for neuron in nn_chem_sub.neurons:\n",
    "            if neuron in measuredNeurons[database]:\n",
    "                nn_chem_sub.neurons[neuron].set_property('amplitude', jsons[database]['trace_array'][measuredNeurons[database][neuron]])\n",
    "                \n",
    "    ## Inputs\n",
    "    inputs = []\n",
    "    time_points = np.arange(num_timepoints)\n",
    "    for inp in input_nodes:\n",
    "        if hasattr(inp, 'amplitude'):\n",
    "            input_value = {t:inp.amplitude[j] for j,t in enumerate(time_points)}\n",
    "            inputs.append(simulator.TimeSeriesInput([inp], input_value))\n",
    "\n",
    "    ## Initialize rate model\n",
    "    rate_model = simulator.RateModel(nn_chem_sub, input_nodes, weights, gains, time_constants, baselines, static_neurons=input_nodes, \\\n",
    "                                            time_points=time_points, inputs=inputs)\n",
    "\n",
    "    node_parameter_bounds =  {'gain': {rn:gain_base[n.name]*gain_lims for n,rn in rate_model.neurons.items() if not n in input_nodes}, \\\n",
    "                                'time_constant': {rn:tconst_base[n.name]*tconst_lims for n,rn in rate_model.neurons.items() if not n in input_nodes},\n",
    "                                'baseline': {rn:base_base[n.name]*base_lim for n,rn in rate_model.neurons.items() if not n in input_nodes}}\n",
    "    \n",
    "    edge_parameter_bounds = {'weight': {(e[0], e[1], e[2]): wts_base[(e[0].name, e[1].name, e[2])]*wts_lim for e in rate_model.edges}}\n",
    "\n",
    "    real = {rate_model.neurons[node]:data['amplitude'] for node,data in nn_chem_sub.nodes(data=True) if 'amplitude' in data}\n",
    "    rate_model.real = real\n",
    "    \n",
    "    vars_to_fit = [rn for rn in real.keys() if not rn in [rate_model.neurons[n] for n in input_nodes]]\n",
    "    o = optimizer.OptunaOptimizer(rate_model, real, optimizer.mean_squared_error, node_parameter_bounds, edge_parameter_bounds, vars_to_fit, num_trials=num_trials, study_name=f\"{database}_{window_length}_{window_step}_{num_trials}\")\n",
    "\n",
    "    ## First pass with short windows to loosely get the parameter space. \n",
    "    for win_ind in np.arange(((num_timepoints-window_length)//window_step) + 1):\n",
    "        \n",
    "        time_points = np.arange(win_ind*window_step, (win_ind*window_step) + window_length )\n",
    "        rate_model.time_points = time_points\n",
    "        \n",
    "        # node_parameter_bounds =  {'gain': {rn:gain_base[n.name]*gain_lims for n,rn in rate_model.neurons.items() if not n in input_nodes}, \\\n",
    "        #                             'time_constant': {rn:tconst_base[n.name]*tconst_lims for n,rn in rate_model.neurons.items() if not n in input_nodes},\n",
    "        #                             'baseline': {rn:base_base[n.name]*base_lim for n,rn in rate_model.neurons.items() if not n in input_nodes}}\n",
    "        # edge_parameter_bounds = {'weight': {(e[0].name.name, e[1].name.name, e[2]): wts_base[(e[0].name.name, e[1].name.name, e[2])]*wts_lim for e in rate_model.edges}}\n",
    "        \n",
    "        # real = {rate_model.neurons[node]:data['amplitude'] for node,data in nn_chem_sub.nodes(data=True) if 'amplitude' in data}\n",
    "        #[win_ind*window_step: (win_ind*window_step) + window_length]\n",
    "\n",
    "        ## Setting parameter bounds for the paramters of interest and set the rest to default to simulate. Use a noisy output to fit.\n",
    "        # o = optimizer.OptunaOptimizer(rate_model, real, optimizer.mean_squared_error, node_parameter_bounds, edge_parameter_bounds, vars_to_fit, num_trials=num_trials)\n",
    "        #o = optimizer.ScipyOptimizer(rate_model, real, optimizer.mean_squared_error, node_parameter_bounds, edge_parameter_bounds, vars_to_fit, num_trials=num_trials)\n",
    "        ## Set vars to fit iteratively in a loop and add network layer by layer?\n",
    "        try:\n",
    "            best_param, best_model = o.optimize()\n",
    "            # print(f\"Optimization successful for {database}. Best parameters: {best_params}\")\n",
    "            # best_fit = best_model.simulate()\n",
    "            # print(\"Simulation successful\")\n",
    "\n",
    "            best_models[database][win_ind] = best_model\n",
    "            best_loss[database][win_ind] = o.study.best_value\n",
    "            best_params[database][win_ind] = best_param\n",
    "            # print(\"Plotting results\") \n",
    "            # plot_rows = [k for k in best_fit.keys() if not str(k.name) in input_neurons and hasattr(nn_chem_sub.neurons[str(k.name)], 'amplitude')]\n",
    "            # f, ax = plt.subplots(figsize=(10,2*len(plot_rows)), nrows=len(plot_rows), sharex=True, layout='constrained')\n",
    "            # # for k, (n, node) in enumerate(nodelist):\n",
    "            # for j,k in enumerate(plot_rows):\n",
    "            #     ax[j].plot(time_points, np.array(nn_chem_sub.neurons[str(k.name)].amplitude)[time_points], label=f'{k.name}-{nn_chem_sub.neurons[str(k.name)].name}', color='gray')\n",
    "            #     ax1 = ax[j]\n",
    "            #     ax1.plot(time_points, best_fit[k], color='orange')\n",
    "            #     utils.simpleaxis(ax[j])\n",
    "            #     ax[j].set_title(f'{np.corrcoef(np.array(nn_chem_sub.neurons[str(k.name)].amplitude)[time_points], best_fit[k])[0,1]}')\n",
    "            #     ax[j].legend(frameon=False)\n",
    "            # f.suptitle(f'{database}')\n",
    "            # plt.show()\n",
    "        except Exception as e:\n",
    "            print(f\"Optimization failed: {e}\")\n",
    "        # best_params, best_model = o.optimize()\n",
    "    print(f\"{datab_in} out of {len(jsons.keys())} done.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for database in jsons:\n",
    "    # with open(f\"{utils.OUTPUT_DIR}/{database}_best_models.json\", 'w') as f:\n",
    "    #     json.dump(best_models[database], f)\n",
    "    with open(f\"{utils.OUTPUT_DIR}/{database}_best_loss.json\", 'w') as f:\n",
    "        json.dump(best_loss[database], f)\n",
    "    with open(f\"{utils.OUTPUT_DIR}/{database}_best_params.json\", 'w') as f:\n",
    "        json.dump(best_params[database], f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f, ax = plt.subplots(figsize=(10,10), nrows=len(jsons.keys()), sharex=True, layout='constrained')\n",
    "for j,database in enumerate(jsons):\n",
    "    loss = []\n",
    "    for k in sorted(best_loss[database].keys()):\n",
    "        loss.append(best_loss[database][k])\n",
    "    ax[j].plot(loss, label=database)\n",
    "    ax[j].legend()\n",
    "    utils.simpleaxis(ax[j])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for inp in inputs:\n",
    "    print(inp, inp.input_neurons[0].process_inputs(0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# num_trials = 10\n",
    "# best_models = {}\n",
    "# for database in jsons.keys():\n",
    "#     ## Subnetwork and optimize\n",
    "#     #nn_chem_sub = nn_chem.subnetwork(connections=edgelist)\n",
    "#     nn_chem_sub = nn_chem.subnetwork(neurons=optim_neurs[database])\n",
    "\n",
    "#     ## Parameter Setup\n",
    "#     inputs = []\n",
    "#     tconstants = [1] *len(nn_chem_sub.nodes)\n",
    "#     input_nodes = [nn_chem_sub.neurons[n] for n in input_neurons]\n",
    "\n",
    "#     weights = {e:1 for e in nn_chem_sub.edges}\n",
    "#     gains = {node:1.0 for node in nn_chem_sub.nodes}\n",
    "#     baselines = {node:0. for node in nn_chem_sub.nodes}\n",
    "#     time_constants = {n:t for n,t in zip(nn_chem_sub.nodes, tconstants)}\n",
    "#     num_timepoints = len(jsons[database]['trace_array'][measuredNeurons[database][list(measuredNeurons[database].keys())[0]]])\n",
    "#     for neuron in nn_chem_sub.neurons:\n",
    "#         if neuron in measuredNeurons[database]:\n",
    "#             nn_chem_sub.neurons[neuron].set_property('amplitude', jsons[database]['trace_array'][measuredNeurons[database][neuron]][:num_timepoints])\n",
    "#     time_points = np.arange(num_timepoints)#jsons[database]['max_t'])\n",
    "\n",
    "#     ## Inputs\n",
    "#     for inp in input_nodes:\n",
    "#         if hasattr(inp, 'amplitude'):\n",
    "#             input_value = {t:inp.amplitude[t] for t in time_points}\n",
    "#             inputs.append(simulator.TimeSeriesInput([inp], input_value))\n",
    "    \n",
    "#     node_parameters={'gain':gains, 'time_constant':time_constants, 'baseline':baselines}\n",
    "#     edge_parameters={'weight':weights}\n",
    "\n",
    "#     ## Initialize rate model\n",
    "#     rate_model = simulator.JaxRateModel(nn_chem_sub, input_nodes, node_parameters=node_parameters, edge_parameters=edge_parameters, static_nodes=input_nodes, \\\n",
    "#                                         time_points=time_points)\n",
    "    \n",
    "#     node_parameter_bounds =  {'gain': {rn:(-1, 1) for n,rn in rate_model.neurons.items() if not n in input_nodes}, \\\n",
    "#                                 'time_constant': {rn:(1, 5) for n,rn in rate_model.neurons.items() if not n in input_nodes},\n",
    "#                                 'baseline': {rn:(0, 2) for n,rn in rate_model.neurons.items() if not n in input_nodes}}\n",
    "#     edge_parameter_bounds = {'weight': {e:(-2, 2) for e in rate_model.edges}}\n",
    "    \n",
    "#     real = {rate_model.neurons[node]:data['amplitude'] for node,data in nn_chem_sub.nodes(data=True) if 'amplitude' in data}\n",
    "#     vars_to_fit = [rn for rn in real.keys() if not rn in [rate_model.neurons[n] for n in input_nodes]]\n",
    "    \n",
    "#     ## Setting parameter bounds for the paramters of interest and set the rest to default to simulate. Use a noisy output to fit.\n",
    "#     #o = optimizer.OptunaOptimizer(rate_model, real, optimizer.mean_squared_error, node_parameter_bounds, edge_parameter_bounds, vars_to_fit, num_trials=num_trials)\n",
    "#     o = optimizer.JaxOptimizer(rate_model, real, optimizer.mean_squared_error, node_parameter_bounds, edge_parameter_bounds, vars_to_fit, num_trials=num_trials)\n",
    "#     ## Set vars to fit iteratively in a loop and add network layer by layer?\n",
    "    \n",
    "#     best_params, best_model = o.optimize()\n",
    "#     best_fit = best_model.simulate()\n",
    "\n",
    "#     best_models[database] = (best_params, best_model)\n",
    "    \n",
    "#     plot_rows = [k for k in best_fit.keys() if not str(k.name) in input_neurons and hasattr(nn_chem_sub.neurons[str(k.name)], 'amplitude')]\n",
    "#     f, ax = plt.subplots(figsize=(10,2*len(plot_rows)), nrows=len(plot_rows), sharex=True, layout='constrained')\n",
    "#     # for k, (n, node) in enumerate(nodelist):\n",
    "#     for j,k in enumerate(plot_rows):\n",
    "#         ax[j].plot(nn_chem_sub.neurons[str(k.name)].amplitude, label=f'{k.name}-{nn_chem_sub.neurons[str(k.name)].name}', color='gray')\n",
    "#         ax1 = ax[j]\n",
    "#         ax1.plot(best_fit[k], color='orange')\n",
    "#         utils.simpleaxis(ax[j])\n",
    "#         ax[j].set_title(f'{np.corrcoef(nn_chem_sub.neurons[str(k.name)].amplitude, best_fit[k])[0,1]}')\n",
    "#         ax[j].legend(frameon=False)\n",
    "#     f.suptitle(f'{database}')\n",
    "#     plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "var_dict = {}\n",
    "for database, (pars, mod) in best_models.items():\n",
    "    for key, val in pars.items():\n",
    "        par, *rest = key.split(':')\n",
    "        if par not in var_dict:\n",
    "            var_dict[par] = {}\n",
    "        if not tuple(rest) in var_dict[par]:\n",
    "            var_dict[par][tuple(rest)] = []\n",
    "        var_dict[par][tuple(rest)].append(val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(nn_chem_sub.neurons)*3 + len(nn_chem_sub.edges)*1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for m in best_models:\n",
    "    sorted_k = sorted(best_models[m].keys())[-1]\n",
    "    model1 = best_models[m][sorted_k]\n",
    "    model1.time_points = np.arange(num_timepoints)\n",
    "    res = model1.simulate()\n",
    "\n",
    "    f, ax = plt.subplots(figsize=(10,2*len(res.keys())), nrows=len(res.keys()), sharex=True, layout='constrained')\n",
    "    # for k, (n, node) in enumerate(nodelist):\n",
    "    for j,k in enumerate(res.keys()):\n",
    "        utils.simpleaxis(ax[j])\n",
    "        if hasattr(nn_chem_sub.neurons[str(k.name)], 'amplitude'):\n",
    "            ax[j].plot(np.arange(num_timepoints), np.array(nn_chem_sub.neurons[str(k.name)].amplitude), color='gray')\n",
    "            ax[j].set_title(f'{np.corrcoef(np.array(nn_chem_sub.neurons[str(k.name)].amplitude)[np.arange(num_timepoints)], res[k])[0,1]}')\n",
    "        # ax[j].plot(time_points, np.array(nn_chem_sub.neurons[str(k.name)].amplitude)[time_points], label=f'{k.name}-{nn_chem_sub.neurons[str(k.name)].name}', color='gray')\n",
    "        ax1 = ax[j]\n",
    "        ax1.plot(np.arange(num_timepoints), res[k], color='orange', label=f'{k.name}-{nn_chem_sub.neurons[str(k.name)].name}')\n",
    "        ax1.legend(frameon=False)\n",
    "    f.suptitle(f'{database}')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f, ax = plt.subplots(figsize=(24,8), nrows= len(var_dict)-1, layout='constrained', sharex=True)\n",
    "for j, (par, vars) in enumerate(sorted(var_dict.items(), key=lambda x:x[0])):\n",
    "    xticks = []\n",
    "    if not par == 'weight':\n",
    "        for k, (n, val) in enumerate(vars.items()):\n",
    "            ax[j].scatter([k]*len(val), val)\n",
    "            xticks.append('-'.join(n))\n",
    "        ax[j].set_xticks(np.arange(len(xticks)), xticks, rotation=45)\n",
    "        utils.simpleaxis(ax[j])\n",
    "        ax[j].set_title(par)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f, ax = plt.subplots(figsize=(24,8), nrows= len(var_dict)-1, layout='constrained', sharex=True)\n",
    "for j, (par, vars) in enumerate(sorted(var_dict.items(), key=lambda x:x[0])):\n",
    "    xticks = []\n",
    "    if not par == 'weight':\n",
    "        for k, (n, val) in enumerate(vars.items()):\n",
    "            ax[j].scatter([k]*len(val), val)\n",
    "            xticks.append('-'.join(n))\n",
    "        ax[j].set_xticks(np.arange(len(xticks)), xticks, rotation=45)\n",
    "        utils.simpleaxis(ax[j])\n",
    "        ax[j].set_title(par)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f, ax = plt.subplots(figsize=(20,12), layout='constrained', nrows=len(var_dict['weight'])//100+1)\n",
    "xticks=[]\n",
    "for j, (n, val) in enumerate(var_dict['weight'].items()):\n",
    "    ax[j//100].scatter([j%100]*len(val), val)\n",
    "    xticks.append('-'.join(n))\n",
    "    # ax[j//100].set_xticks(np.arange(len(xticks)), xticks, rotation=45)\n",
    "utils.simpleaxis(ax)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f, ax = plt.subplots(figsize=(36,8), layout='constrained', nrows=2)\n",
    "xticks_1=[]\n",
    "xticks_2=[]\n",
    "k1=0\n",
    "k2 = 0\n",
    "for j, (n, val) in enumerate(var_dict['weight'].items()):\n",
    "    if n[1] in command_interneurons:\n",
    "        ax[0].scatter([k1]*len(val), val)\n",
    "        xticks_1.append('-'.join(n))\n",
    "        k1+=1\n",
    "    if n[0] in command_interneurons:\n",
    "        ax[1].scatter([k2]*len(val), val)\n",
    "        xticks_2.append('-'.join(n))\n",
    "        k2+=1\n",
    "ax[0].set_xticks(np.arange(len(xticks_1)), xticks_1, rotation=45, fontsize='x-large')\n",
    "ax[1].set_xticks(np.arange(len(xticks_2)), xticks_2, rotation=45, fontsize='x-large')\n",
    "utils.simpleaxis(ax)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "min_motif = ['1.1', '1.2', '2.1', '2.2', '3.1']\n",
    "tconstants = [1, 1, 1, 1,1,1,1]\n",
    "input_nodes = [min_motif[0]]\n",
    "\n",
    "weights = {e:1 for e in hseq.edges}\n",
    "gains = {node:1.0 for node in hseq.nodes}\n",
    "baselines = {node:0. for node in hseq.nodes}\n",
    "time_constants = {n:t for n,t in zip(hseq.nodes, tconstants)}\n",
    "\n",
    "# countdown = 10\n",
    "for database in jsons.keys():\n",
    "    nn_chem_sub = nn_chem.subnetwork(connections=all_edges)\n",
    "    all_ffgs = nn_chem_sub.search_motifs(hseq)\n",
    "    num_timepoints = len(jsons[database]['trace_array'][measuredNeurons[database][list(measuredNeurons[database].keys())[0]]])\n",
    "    for neuron in nn_chem_sub.neurons:\n",
    "        if neuron in measuredNeurons[database]:\n",
    "            nn_chem_sub.neurons[neuron].set_property('amplitude', jsons[database]['trace_array'][measuredNeurons[database][neuron]])\n",
    "    \n",
    "    by_motif = {}\n",
    "    for j,ffg in enumerate(all_ffgs):\n",
    "        nodelist = []\n",
    "        for edge in sorted(edges):\n",
    "            if hasattr(nn_chem_sub.neurons[ffg[edge][0].name], 'amplitude') and hasattr(nn_chem_sub.neurons[ffg[edge][1].name], 'amplitude'):\n",
    "                nodelist+= [(edge[0], ffg[edge][0].name), (edge[1], ffg[edge][1].name)]\n",
    "        nodelist = sorted(set(nodelist))\n",
    "        if nodelist:# and countdown>0:\n",
    "            if all(n in list(zip(*nodelist))[0] for n in min_motif):\n",
    "                \n",
    "                cedne.GraphMap(ffg, hseq, nn_chem_sub, map_type='edge')\n",
    "                inputs = []\n",
    "                time_points = np.arange(0,jsons[database]['max_t'])\n",
    "                for inp in input_nodes:\n",
    "                    if hasattr(nn_chem_sub.neurons[hseq.nodes[inp]['map'].name], 'amplitude'):\n",
    "                        input_value = {t:nn_chem_sub.neurons[hseq.nodes[inp]['map'].name].amplitude[t] for t in time_points}\n",
    "                        inputs.append(simulator.TimeSeriesInput(input_nodes, input_value))\n",
    "                rate_model = simulator.RateModel(hseq, input_nodes, weights, gains, time_constants, baselines, static_nodes=input_nodes, time_points=time_points, inputs=inputs)\n",
    "                \n",
    "                node_parameter_bounds =  {'gain': {rn:(0, 5) for n,rn in rate_model.neurons.items() if not n in input_nodes}, 'time_constant': {rn:(0, 20) for n,rn in rate_model.neurons.items() if not n in input_nodes}, 'baseline': {rn:(0, 3) for n,rn in rate_model.neurons.items() if not n in input_nodes}}\n",
    "                edge_parameter_bounds = {'weight': {e:(-10, 10) for e in rate_model.edges}}\n",
    "                \n",
    "                real = {rate_model.neurons[node]:data['map'].amplitude for node,data in hseq.nodes(data=True) if hasattr(data['map'], 'amplitude')}\n",
    "                vars_to_fit = [rn for rn in real.keys() if not rn in input_nodes]\n",
    "                \n",
    "                ## Setting parameter bounds for the paramters of interest and set the rest to default to simulate. Use a noisy output to fit.\n",
    "                o = optimizer.OptunaOptimizer(rate_model, real, optimizer.mean_squared_error, node_parameter_bounds, edge_parameter_bounds, vars_to_fit, num_trials=1e3)\n",
    "                best_params, best_model = o.optimize()\n",
    "                best_fit = best_model.simulate()\n",
    "                \n",
    "                f, ax = plt.subplots(figsize=(10,2*len(hseq.nodes)), nrows=len(hseq.nodes), sharex=True)\n",
    "                # for k, (n, node) in enumerate(nodelist):\n",
    "                for j,k in enumerate(best_fit.keys()):\n",
    "                    if k.name in list(zip(*nodelist))[0]:\n",
    "                        ax[j].plot(nn_chem_sub.neurons[hseq.nodes[k.name]['map'].name].amplitude, label=f'{k.name}-{hseq.nodes[k.name]['map'].name}', color='gray')\n",
    "                        ax1 = ax[j]\n",
    "                        ax1.plot(best_fit[k], color='orange')\n",
    "                        utils.simpleaxis(ax[j])\n",
    "                        ax[j].set_title(f'{np.corrcoef(nn_chem_sub.neurons[hseq.nodes[k.name]['map'].name].amplitude, best_fit[k])[0,1]}')\n",
    "                        ax[j].legend(frameon=False)\n",
    "                f.suptitle(f'{database}')\n",
    "                plt.show()\n",
    "                # countdown-=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "triads = utils.return_triads()\n",
    "G = triads['030T']\n",
    "weights = {(1, 3): -3., (3, 2): -1, (1, 2): -3}\n",
    "\n",
    "input_nodes = [1]\n",
    "gains = {node:1.0 for node in G.nodes}\n",
    "tconstants = [10, 10, 1]\n",
    "time_constants = {n:t for n,t in zip(G.nodes, tconstants)}\n",
    "rate_model = simulator.RateModel(G, input_nodes, weights, gains, time_constants, static_nodes=input_nodes)\n",
    "\n",
    "initial_rates = [0., 0., 0.]\n",
    "max_t = 90\n",
    "time_points = np.linspace(0, max_t, 451)\n",
    "\n",
    "inp1_value = 1\n",
    "input_value = {t:inp1_value*np.sin((t/max_t)*2*np.pi) for t in time_points}\n",
    "inp_vals = [input_value[t] for t in time_points]\n",
    "input1= simulator.TimeSeriesInput(input_nodes, input_value)\n",
    "\n",
    "inputs = [input1]\n",
    "\n",
    "rates = rate_model.simulate(time_points, inputs)\n",
    "\n",
    "f = utils.plot_simulation_results((rate_model, inputs, rates), twinx=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "k"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
