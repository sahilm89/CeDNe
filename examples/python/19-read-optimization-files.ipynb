{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import optuna\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import cedne\n",
    "from cedne import utils\n",
    "from cedne import simulator\n",
    "import numpy as np\n",
    "import json\n",
    "import scipy.stats as ss\n",
    "import psycopg2\n",
    "import subprocess\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_valid_pgdata(PGDATA):\n",
    "    \"\"\"Checks if PGDATA is a valid PostgreSQL cluster directory.\"\"\"\n",
    "    return os.path.exists(PGDATA) and os.path.exists(os.path.join(PGDATA, \"PG_VERSION\"))\n",
    "\n",
    "def initialize_postgres(PGDATA):\n",
    "    \"\"\"Initializes a new PostgreSQL cluster if PGDATA is not valid.\"\"\"\n",
    "    print(f\"Initializing PostgreSQL in {PGDATA}...\")\n",
    "    try:\n",
    "        subprocess.run([\"initdb\", \"-D\", PGDATA], check=True)\n",
    "        print(\"PostgreSQL initialized successfully.\")\n",
    "        return True\n",
    "    except subprocess.CalledProcessError as e:\n",
    "        print(f\"Error initializing PostgreSQL: {e}\")\n",
    "        return False\n",
    "    \n",
    "def is_postgres_running(port=5433):\n",
    "    \"\"\"Check if PostgreSQL is already running on a given port.\"\"\"\n",
    "    result = subprocess.getoutput(f\"lsof -i :{port} | grep postgres\")\n",
    "    return bool(result.strip())  # Returns True if something is running on the port\n",
    "\n",
    "def stop_postgres(PGDATA):\n",
    "    \"\"\"Stops PostgreSQL using pg_ctl within Python.\"\"\"\n",
    "    print(f\"Stopping PostgreSQL using {PGDATA}... and dropping table.\")\n",
    "    try:\n",
    "        # Drop table psql -U sahilmoza -h localhost -p 5433 -c \"DROP DATABASE IF EXISTS cedne_optimization_optuna;\"\n",
    "        user = os.getenv(\"USER\") or \"postgres\"  # Get the system user dynamically\n",
    "        db_name = \"cedne_optimization_optuna\"\n",
    "        subprocess.run([\"psql\", \"-U\", user, \"-h\", \"localhost\", \"-p\", \"5433\", \"-c\", f\"DROP DATABASE IF EXISTS {db_name};\"], check=True)\n",
    "        subprocess.run([\"pg_ctl\", \"-D\", PGDATA, \"stop\"], check=True)\n",
    "        time.sleep(3)\n",
    "        result = subprocess.run([\"pg_ctl\", \"-D\", PGDATA, \"status\"], capture_output=True, text=True)\n",
    "        if \"server is running\" not in result.stdout:\n",
    "            print(\"PostgreSQL stopped successfully.\")\n",
    "            return True\n",
    "        else:\n",
    "            print(\"PostgreSQL did not stop properly.\")\n",
    "            return False\n",
    "    except subprocess.CalledProcessError as e:\n",
    "        print(f\"Error stopping PostgreSQL: {e}\")\n",
    "        return False\n",
    "    \n",
    "def start_postgres(PGDATA, PG_PORT):\n",
    "    \"\"\"Starts PostgreSQL safely, checking for existing processes and fixing permissions.\"\"\"\n",
    "    \n",
    "    if is_postgres_running(PG_PORT):\n",
    "        print(f\"PostgreSQL is already running on port {PG_PORT}. Skipping start.\")\n",
    "        return True\n",
    "\n",
    "    if not is_valid_pgdata(PGDATA):\n",
    "        print(f\"PGDATA {PGDATA} is not a valid PostgreSQL cluster. Initializing...\")\n",
    "        if not initialize_postgres(PGDATA):\n",
    "            return False\n",
    "\n",
    "    print(f\"Starting PostgreSQL on port {PG_PORT} using {PGDATA}...\")\n",
    "\n",
    "    # Fix permissions before starting\n",
    "    subprocess.run([\"sudo\", \"chown\", \"-R\", os.getenv(\"USER\"), PGDATA], check=False)\n",
    "    subprocess.run([\"chmod\", \"-R\", \"700\", PGDATA], check=False)\n",
    "\n",
    "    try:\n",
    "        subprocess.run([\"pg_ctl\", \"-D\", PGDATA, \"start\", \"-o\", f\"-p {PG_PORT}\"], check=True)\n",
    "        time.sleep(3)\n",
    "        \n",
    "        # Check if PostgreSQL started successfully\n",
    "        if is_postgres_running(PG_PORT):\n",
    "            print(\"PostgreSQL started successfully.\")\n",
    "            return True\n",
    "        else:\n",
    "            print(\"PostgreSQL did not start properly. Checking for conflicts...\")\n",
    "            stop_postgres(PGDATA)  # Stop any conflicting instances\n",
    "            return False\n",
    "    except subprocess.CalledProcessError:\n",
    "        print(f\"Error starting PostgreSQL at {PGDATA}. Checking for conflicting processes...\")\n",
    "\n",
    "        # Kill any existing PostgreSQL process on the port\n",
    "        process_output = subprocess.getoutput(f\"lsof -i :{PG_PORT}\")\n",
    "        for line in process_output.split(\"\\n\"):\n",
    "            if \"postgres\" in line:\n",
    "                pid = line.split()[1]\n",
    "                subprocess.run([\"kill\", \"-9\", pid], check=False)\n",
    "                print(f\"Killed process {pid} running on port {PG_PORT}\")\n",
    "\n",
    "        # Retry starting PostgreSQL\n",
    "        try:\n",
    "            subprocess.run([\"pg_ctl\", \"-D\", PGDATA, \"start\", \"-o\", f\"-p {PG_PORT}\"], check=True)\n",
    "            return is_postgres_running(PG_PORT)\n",
    "        except subprocess.CalledProcessError as e:\n",
    "            print(f\"Failed to start PostgreSQL after retry: {e}\")\n",
    "            return False\n",
    "\n",
    "def ensure_role_exists(role=\"smoza\", superuser=True, createdb=True, createrole=True, port=5433, host=\"localhost\"):\n",
    "    \"\"\"Ensures the PostgreSQL role exists. Creates it if missing.\"\"\"\n",
    "    try:\n",
    "        user = os.getenv(\"USER\") or \"postgres\"  # Get the current system user\n",
    "        conn = psycopg2.connect(dbname=\"postgres\", user=user, host=host, port=port)\n",
    "        conn.autocommit = True\n",
    "        cursor = conn.cursor()\n",
    "\n",
    "        # Check if role exists\n",
    "        cursor.execute(\"SELECT 1 FROM pg_roles WHERE rolname = %s;\", (role,))\n",
    "        if cursor.fetchone():\n",
    "            print(f\"Role '{role}' already exists.\")\n",
    "        else:\n",
    "            role_query = f\"CREATE ROLE {role} WITH LOGIN\"\n",
    "            if superuser:\n",
    "                role_query += \" SUPERUSER\"\n",
    "            if createdb:\n",
    "                role_query += \" CREATEDB\"\n",
    "            if createrole:\n",
    "                role_query += \" CREATEROLE\"\n",
    "            \n",
    "            cursor.execute(role_query + \";\")\n",
    "            print(f\"Role '{role}' created successfully.\")\n",
    "\n",
    "        cursor.close()\n",
    "        conn.close()\n",
    "    except psycopg2.Error as e:\n",
    "        print(f\"Error ensuring role '{role}' exists: {e}\")\n",
    "\n",
    "def database_exists(db_name, port=5433, host=\"localhost\"):\n",
    "    \"\"\"Checks if a PostgreSQL database exists.\"\"\"\n",
    "    try:\n",
    "        user = os.getenv(\"USER\")  # Dynamically get the user\n",
    "        ensure_role_exists(role=user)  # Ensure the role exists before checking\n",
    "\n",
    "        conn = psycopg2.connect(dbname=\"postgres\", user=user, host=host, port=port)\n",
    "        conn.autocommit = True\n",
    "        cursor = conn.cursor()\n",
    "        cursor.execute(\"SELECT 1 FROM pg_database WHERE datname = %s;\", (db_name,))\n",
    "        exists = cursor.fetchone() is not None\n",
    "        cursor.close()\n",
    "        conn.close()\n",
    "        return exists\n",
    "    except psycopg2.Error as e:\n",
    "        print(f\"Error checking database existence: {e}\")\n",
    "        return False\n",
    "\n",
    "# def restore_pgdata(PGDATA, PG_PORT, DUMP_NAME, root_path):\n",
    "#     \"\"\"Restores PostgreSQL data from a dump file.\"\"\"\n",
    "#     print(\"Restoring folder from dump...\")\n",
    "#     try:\n",
    "#         db_name = \"cedne_optimization_optuna\"\n",
    "#         user = os.getenv(\"USER\") or \"postgres\"  # Get the system user dynamically\n",
    "#         dump_path = os.path.join(root_path, DUMP_NAME)  # Dump file remains in root_path\n",
    "\n",
    "#         # Ensure PostgreSQL is running\n",
    "#         if not start_postgres(PGDATA, PG_PORT):\n",
    "#             return False\n",
    "\n",
    "#         # Ensure the \"smoza\" role exists before restoring\n",
    "#         ensure_role_exists(\"smoza\")\n",
    "\n",
    "#         # Ensure the database exists before restoring\n",
    "#         if not database_exists(db_name, port=PG_PORT):\n",
    "#             subprocess.run([\"createdb\", \"-U\", user, \"-h\", \"localhost\", \"-p\", str(PG_PORT), db_name], check=True)\n",
    "\n",
    "#         # # Restore from the dump file in root_path (not inside PGDATA)\n",
    "#         # subprocess.run([\"pg_restore\", \"-U\", user, \"-h\", \"localhost\", \"-p\", str(PG_PORT), \"-d\", db_name,\n",
    "#         #                 dump_path], check=True)\n",
    "#         # Restore using --no-owner and excluding large unused tables\n",
    "#         subprocess.run([\"pg_restore\", \"-U\", user, \"-h\", \"localhost\", \"-p\", str(PG_PORT), \"-d\", db_name,\n",
    "#                         \"--no-owner\", dump_path], check=True)\n",
    "\n",
    "#         # Optimize Database After Restore\n",
    "#         subprocess.run([\"psql\", \"-U\", user, \"-h\", \"localhost\", \"-p\", str(PG_PORT), \"-d\", db_name, \"-c\", \"VACUUM FULL;\"], check=True)\n",
    "#         subprocess.run([\"psql\", \"-U\", user, \"-h\", \"localhost\", \"-p\", str(PG_PORT), \"-d\", db_name, \"-c\", \"ANALYZE;\"], check=True)\n",
    "\n",
    "#         print(\"Database restored and optimized successfully.\")\n",
    "\n",
    "#         #print(\"Database restored successfully.\")\n",
    "#         return True\n",
    "#     except subprocess.CalledProcessError as e:\n",
    "#         print(f\"Problem restoring: {e}\")\n",
    "#         return False\n",
    "\n",
    "def restore_pgdata(PGDATA, PG_PORT, DUMP_NAME, root_path):\n",
    "    \"\"\"Restores PostgreSQL data from a dump file after ensuring a clean state.\"\"\"\n",
    "    print(\"Restoring PostgreSQL database from dump...\")\n",
    "\n",
    "    try:\n",
    "        db_name = \"cedne_optimization_optuna\"\n",
    "        user = os.getenv(\"USER\") or \"postgres\"\n",
    "        dump_path = os.path.join(root_path, DUMP_NAME)\n",
    "\n",
    "        # Ensure PostgreSQL is running\n",
    "        if not start_postgres(PGDATA, PG_PORT):\n",
    "            return False\n",
    "\n",
    "        # Ensure the \"smoza\" role exists before restoring\n",
    "        ensure_role_exists(\"smoza\")\n",
    "\n",
    "        # Terminate all active connections before dropping the database\n",
    "        print(f\"Terminating active connections to {db_name}...\")\n",
    "        subprocess.run([\n",
    "            \"psql\", \"-U\", user, \"-h\", \"localhost\", \"-p\", str(PG_PORT), \"-d\", \"postgres\",\n",
    "            \"-c\", f\"SELECT pg_terminate_backend(pid) FROM pg_stat_activity WHERE datname = '{db_name}';\"\n",
    "        ], check=False)\n",
    "\n",
    "        # Drop and recreate the database\n",
    "        print(f\"Dropping existing database {db_name}...\")\n",
    "        subprocess.run([\"psql\", \"-U\", user, \"-h\", \"localhost\", \"-p\", str(PG_PORT), \"-d\", \"postgres\",\n",
    "                        \"-c\", f\"DROP DATABASE IF EXISTS {db_name};\"], check=True)\n",
    "\n",
    "        print(f\"Creating new database {db_name}...\")\n",
    "        subprocess.run([\"psql\", \"-U\", user, \"-h\", \"localhost\", \"-p\", str(PG_PORT), \"-d\", \"postgres\",\n",
    "                        \"-c\", f\"CREATE DATABASE {db_name};\"], check=True)\n",
    "\n",
    "        # Restore only the pre-data section, which includes custom types\n",
    "        # print(\"Restoring custom types...\")\n",
    "        # subprocess.run([\"pg_restore\", \"-U\", user, \"-h\", \"localhost\", \"-p\", str(PG_PORT), \"-d\", db_name,\n",
    "        #                 \"--section=pre-data\", \"--no-owner\", dump_path], check=True)\n",
    "\n",
    "        # # Restore schema only\n",
    "        # print(\"Restoring database schema...\")\n",
    "        # subprocess.run([\"pg_restore\", \"-U\", user, \"-h\", \"localhost\", \"-p\", str(PG_PORT), \"-d\", db_name,\n",
    "        #                 \"--schema-only\", \"--no-owner\", dump_path], check=True)\n",
    "\n",
    "        # # Restore data only\n",
    "        # print(\"Restoring database data...\")\n",
    "        # subprocess.run([\"pg_restore\", \"-U\", user, \"-h\", \"localhost\", \"-p\", str(PG_PORT), \"-d\", db_name,\n",
    "        #                 \"--data-only\", \"--no-owner\", dump_path], check=True)\n",
    "\n",
    "        # # Optimize the database after restore\n",
    "        # print(\"Optimizing database after restore...\")\n",
    "        # subprocess.run([\"psql\", \"-U\", user, \"-h\", \"localhost\", \"-p\", str(PG_PORT), \"-d\", db_name, \"-c\", \"VACUUM FULL;\"], check=True)\n",
    "        # subprocess.run([\"psql\", \"-U\", user, \"-h\", \"localhost\", \"-p\", str(PG_PORT), \"-d\", db_name, \"-c\", \"ANALYZE;\"], check=True)\n",
    "\n",
    "\n",
    "        subprocess.run([\n",
    "            \"pg_restore\", \"-U\", \"sahilmoza\", \"-h\", \"localhost\", \"-p\", \"5433\", \"-d\", \"cedne_optimization_optuna\",\n",
    "            \"--no-owner\", \"--clean\", \"--if-exists\"\n",
    "        ], check=False)\n",
    "\n",
    "        print(\"Database restored and optimized successfully.\")\n",
    "        return True\n",
    "\n",
    "    except subprocess.CalledProcessError as e:\n",
    "        print(f\"Problem restoring: {e}\")\n",
    "        return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_parameters(params):\n",
    "    parsed_params = {}\n",
    "    nrs = []\n",
    "    for key in params.keys():\n",
    "        if key.startswith('gain'):\n",
    "            _, n = key.split(':')\n",
    "            if 'gain' not in parsed_params:\n",
    "                parsed_params['gain'] = {}\n",
    "            parsed_params['gain'][n] = params[key]\n",
    "            nrs.append(n)\n",
    "        elif key.startswith('time_constant'):\n",
    "            _, n = key.split(':')\n",
    "            if 'time_constant' not in parsed_params:\n",
    "                parsed_params['time_constant'] = {}\n",
    "            parsed_params['time_constant'][n] = params[key]\n",
    "        elif key.startswith('baseline'):\n",
    "            _, n = key.split(':')\n",
    "            if 'baseline' not in parsed_params:\n",
    "                parsed_params['baseline'] = {}\n",
    "            parsed_params['baseline'][n] = params[key]\n",
    "        elif key.startswith('weight'):\n",
    "            _, n1, n2, _ = key.split(':')\n",
    "            if 'weight' not in parsed_params:\n",
    "                parsed_params['weight'] = {}\n",
    "            parsed_params['weight'][(n1,n2)] = params[key]\n",
    "            nrs.append(n1)\n",
    "            nrs.append(n2)\n",
    "    nrs = list(set(nrs))\n",
    "    return parsed_params, nrs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "jsons = {}\n",
    "for js in os.listdir('/Users/sahilmoza/Documents/Postdoc/Yun Zhang/data/SteveFlavell-NeuroPAL-Cell/Control/'):\n",
    "    with open (\"/Users/sahilmoza/Documents/Postdoc/Yun Zhang/data/SteveFlavell-NeuroPAL-Cell/Control/{}\".format(js), 'r') as f:\n",
    "        jsons[js] = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "measuredNeurons = {}\n",
    "optim_neurs = {js:[] for js in jsons.keys()}\n",
    "for js, p in jsons.items():\n",
    "    sortedKeys = sorted ([int(x) for x in (p['labeled'].keys())])\n",
    "    labelledNeurons = {p['labeled'][str(x)]['label']:x for x in sortedKeys if not '?' in p['labeled'][str(x)]['label']} # Removing unsure hits\n",
    "    measuredNeurons[js] = {m:i for i,m in enumerate(set(labelledNeurons))}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w = utils.makeWorm(chem_only=True)\n",
    "nn_chem = w.networks[\"Neutral\"]\n",
    "root_path = '/Users/sahilmoza/Documents/Codes/CEDNe/tmp/7465167'\n",
    "# Connect to the restored PostgreSQL database\n",
    "# PGDATA = \"pgdata_5406695_8\"  # Path to your PostgreSQL data directory\n",
    "PGUSER = os.getenv(\"USER\")  # User to connect to PostgreSQL\n",
    "PG_PORT = \"5433\"  # Port to use"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PGUSER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_params = {}\n",
    "\n",
    "for dump_file in os.listdir(root_path):\n",
    "    if not dump_file.endswith(\".dump\"):\n",
    "        continue  # Skip non-dump files\n",
    "    #for DUMP_NAME in os.listdir(root_path):\n",
    "    PGDATA = os.path.splitext(dump_file)[0]  # Extract filename without extension\n",
    "    PGDATA_PATH = os.path.join(root_path, PGDATA)\n",
    "    DUMP_PATH = os.path.join(root_path, dump_file)\n",
    "\n",
    "    if not os.path.exists(PGDATA_PATH):\n",
    "        os.mkdir(PGDATA_PATH)\n",
    "        initialize_postgres(PGDATA_PATH)\n",
    "        #os.rename(DUMP_PATH, os.path.join(PGDATA_PATH, dump_file))\n",
    "    #start_postgres(root_path, PG_PORT)\n",
    "    restore_pgdata(PGDATA_PATH, PG_PORT, DUMP_NAME=dump_file, root_path=root_path)\n",
    "    storage = f\"postgresql://{PGUSER}@/cedne_optimization_optuna?host=/tmp&port={PG_PORT}\"\n",
    "    \n",
    "    # Load studies and extract trials\n",
    "    try:\n",
    "        rdb_storage = optuna.storages.RDBStorage(storage)\n",
    "        frozen_studies = rdb_storage.get_all_studies()\n",
    "        for frozen_study in frozen_studies:\n",
    "            study = optuna.load_study(study_name=frozen_study.study_name, storage=storage)\n",
    "            trials = study.trials\n",
    "            if len(trials):\n",
    "                # Extract completed trials\n",
    "                completed_trials = [(t.number, t.value) for t in trials if t.state == optuna.trial.TrialState.COMPLETE]\n",
    "                trial_numbers, loss_values = zip(*completed_trials) if completed_trials else ([], [])\n",
    "\n",
    "                print(f\"Study: {frozen_study.study_name}, Completed Trials: {len(trial_numbers)}\")\n",
    "\n",
    "                best_params[frozen_study.study_name] = study.best_params\n",
    "                params, neurons = parse_parameters(best_params[frozen_study.study_name])\n",
    "                \n",
    "                nn_chem_sub = nn_chem.subnetwork(neuron_names=neurons)\n",
    "\n",
    "                database = frozen_study.study_name.split('Atanas_2023_Control_')[-1].split('_json')[0]+ '.json'\n",
    "                print(database)\n",
    "                \n",
    "                for neuron in nn_chem_sub.neurons:\n",
    "                    if neuron in neurons:\n",
    "                        nn_chem_sub.neurons[neuron].set_property('amplitude', jsons[database]['trace_array'][measuredNeurons[database][neuron]])\n",
    "\n",
    "                num_timepoints = len(jsons[database]['trace_array'][measuredNeurons[database][list(measuredNeurons[database].keys())[0]]])\n",
    "                input_nodes = [nn_chem_sub.neurons[n] for n in nn_chem_sub.neurons if nn_chem_sub.neurons[n].type == 'sensory']\n",
    "                inputs = []\n",
    "                time_points = np.arange(num_timepoints)\n",
    "                for inp in input_nodes:\n",
    "                    if hasattr(inp, 'amplitude'):\n",
    "                        input_value = {t:inp.amplitude[j] for j,t in enumerate(time_points)}\n",
    "                        inputs.append(simulator.TimeSeriesInput([inp], input_value))\n",
    "\n",
    "                baseline = {nn_chem_sub.neurons[n]:0 for n in nn_chem_sub.neurons}\n",
    "                gains = {nn_chem_sub.neurons[n]:1 for n in nn_chem_sub.neurons}\n",
    "                time_constants = {nn_chem_sub.neurons[n]:1 for n in nn_chem_sub.neurons}\n",
    "                weights = {(nn_chem_sub.neurons[e[0].name], nn_chem_sub.neurons[e[1].name]):1 for e in nn_chem_sub.edges}\n",
    "\n",
    "                baseline.update({nn_chem_sub.neurons[n]:v for n,v in params['baseline'].items()})\n",
    "                gains.update({nn_chem_sub.neurons[n]:v for n,v in params['gain'].items()})\n",
    "                time_constants.update({nn_chem_sub.neurons[n]:v for n,v in params['time_constant'].items()})\n",
    "                weights.update({(nn_chem_sub.neurons[e[0]],nn_chem_sub.neurons[e[1]]):v for e,v in params['weight'].items()})\n",
    "\n",
    "                rate_model = simulator.RateModel(nn_chem_sub, input_nodes, weights, gains, time_constants, baseline, static_neurons=input_nodes, \\\n",
    "                                                        time_points=time_points, inputs=inputs)\n",
    "                rate_model.time_points = time_points\n",
    "                res = rate_model.simulate()\n",
    "\n",
    "                f, ax = plt.subplots(figsize=(10,2*len(res.keys())), nrows=len(res.keys()), sharex=True, layout='constrained')\n",
    "                # for k, (n, node) in enumerate(nodelist):\n",
    "                for j,k in enumerate(res.keys()):\n",
    "                    utils.simpleaxis(ax[j])\n",
    "                    if hasattr(nn_chem_sub.neurons[str(k.name)], 'amplitude'):\n",
    "                        ax[j].plot(np.arange(num_timepoints), np.array(nn_chem_sub.neurons[str(k.name)].amplitude), color='gray')\n",
    "                        ax[j].set_title(f'{np.corrcoef(np.array(nn_chem_sub.neurons[str(k.name)].amplitude)[np.arange(num_timepoints)], res[k])[0,1]}')\n",
    "                    # ax[j].plot(time_points, np.array(nn_chem_sub.neurons[str(k.name)].amplitude)[time_points], label=f'{k.name}-{nn_chem_sub.neurons[str(k.name)].name}', color='gray')\n",
    "                    ax1 = ax[j]\n",
    "                    ax1.plot(np.arange(num_timepoints), res[k], color='orange', label=f'{k.name}-{nn_chem_sub.neurons[str(k.name)].name}')\n",
    "                    ax1.legend(frameon=False)\n",
    "                f.suptitle(f'{database}')\n",
    "                plt.show()\n",
    "                # Plot loss over trials\n",
    "                # f, ax = plt.subplots(figsize=(5,3))\n",
    "                # ax.plot(trial_numbers, loss_values, marker=\"o\", linestyle=\"-\", color=\"gray\")\n",
    "                # ax.set_yscale(\"log\")\n",
    "                # ax.set_xlabel(\"Trial Number\")\n",
    "                # ax.set_ylabel(\"Log Loss\")\n",
    "                # # ax.set_title(f\"{study_name} Loss over Trials\")\n",
    "                # utils.simpleaxis(ax)\n",
    "                # plt.show()\n",
    "            else:\n",
    "                raise (f\"Study: {frozen_study.study_name}, No Completed Trials\")\n",
    "    except psycopg2.Error as e:\n",
    "        print(f\"Error connecting to database: {e}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Unexpected error: {e}\")\n",
    "    time.sleep(3)\n",
    "    stop_postgres(root_path + PGDATA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "study.trials"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "jsons.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "params['weight']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_postgres(root_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "counter = 0\n",
    "optimizable_neurs = {}\n",
    "for n in nn_chem.neurons:\n",
    "    # conn_neu_names = set([m2.name for m1,m2,id in nn_chem.neurons[n].outgoing()]) | set([m1.name for m1,m2,id in nn_chem.neurons[n].incoming()])\n",
    "    conn_neu_names = set([m1.name for m1,m2,id in nn_chem.neurons[n].incoming()])\n",
    "    fracs = []\n",
    "    pres = []\n",
    "    for database,p in jsons.items():\n",
    "        if not database in optimizable_neurs.keys():\n",
    "            optimizable_neurs[database] = []\n",
    "        sortedKeys = sorted ([int(x) for x in (p['labeled'].keys())])\n",
    "        labelledNeurons = {p['labeled'][str(x)]['label']:x for x in sortedKeys if not '?' in p['labeled'][str(x)]['label']}\n",
    "        if len(conn_neu_names):\n",
    "            frac = len(set(labelledNeurons.keys()) & conn_neu_names)/len(conn_neu_names)\n",
    "            if n in labelledNeurons.keys() and frac>0.33:\n",
    "                fracs.append(frac)\n",
    "                pres.append(n in set(labelledNeurons.keys()))\n",
    "                optimizable_neurs[database].append(n)\n",
    "    if len(fracs)>0:\n",
    "        counter+=1\n",
    "        print(counter, n, nn_chem.neurons[n].type, list(zip(pres,fracs)))\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "jsons = {}\n",
    "for js in os.listdir('/Users/sahilmoza/Documents/Postdoc/Yun Zhang/data/SteveFlavell-NeuroPAL-Cell/Control/'):\n",
    "    with open (\"/Users/sahilmoza/Documents/Postdoc/Yun Zhang/data/SteveFlavell-NeuroPAL-Cell/Control/{}\".format(js), 'r') as f:\n",
    "        jsons['/n/home05/smoza/CEDNe/data_sources/downloads/Atanas_2023/Control/' + js+'_50_25_64'] = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_params = {study.study_name: study.best_params}\n",
    "neurons = {}\n",
    "for study_name in best_params.keys():\n",
    "    nrs = []\n",
    "    params = best_params[study_name]\n",
    "    for key in params.keys():\n",
    "        if key.startswith('weight'):\n",
    "            _, n1, n2, _ = key.split(':')\n",
    "            nrs.append(n1)\n",
    "            nrs.append(n2)\n",
    "            nrs = list(set(nrs))\n",
    "    neurons[study_name] = nrs\n",
    "\n",
    "database = study.study_name\n",
    "nn_chem_sub = nn_chem.subnetwork(neuron_names=neurons[database])\n",
    "for neuron in nn_chem_sub.neurons:\n",
    "    if neuron in neurons[database]:\n",
    "        nn_chem_sub.neurons[neuron].set_property('amplitude', jsons[database]['trace_array'][measuredNeurons[database][neuron]])\n",
    "\n",
    "num_timepoints = len(jsons[database]['trace_array'][measuredNeurons[database][list(measuredNeurons[database].keys())[0]]])\n",
    "input_nodes = [nn_chem_sub.neurons[n] for n in nn_chem_sub.neurons if nn_chem_sub.neurons[n].type == 'sensory']\n",
    "inputs = []\n",
    "time_points = np.arange(num_timepoints)\n",
    "for inp in input_nodes:\n",
    "    if hasattr(inp, 'amplitude'):\n",
    "        input_value = {t:inp.amplitude[j] for j,t in enumerate(time_points)}\n",
    "        inputs.append(simulator.TimeSeriesInput([inp], input_value))\n",
    "\n",
    "params = parse_parameters(best_params[database])\n",
    "\n",
    "baseline = {nn_chem_sub.neurons[n]:0 for n in nn_chem_sub.neurons}\n",
    "gains = {nn_chem_sub.neurons[n]:1 for n in nn_chem_sub.neurons}\n",
    "time_constants = {nn_chem_sub.neurons[n]:1 for n in nn_chem_sub.neurons}\n",
    "weights = {(nn_chem_sub.neurons[e[0].name], nn_chem_sub.neurons[e[1].name]):1 for e in nn_chem_sub.edges}\n",
    "\n",
    "baseline.update({nn_chem_sub.neurons[n]:v for n,v in params['baseline'].items()})\n",
    "gains.update({nn_chem_sub.neurons[n]:v for n,v in params['gain'].items()})\n",
    "time_constants.update({nn_chem_sub.neurons[n]:v for n,v in params['time_constant'].items()})\n",
    "weights.update({(nn_chem_sub.neurons[e[0]],nn_chem_sub.neurons[e[1]]):v for e,v in params['weight'].items()})\n",
    "\n",
    "rate_model = simulator.RateModel(nn_chem_sub, input_nodes, weights, gains, time_constants, baseline, static_neurons=input_nodes, \\\n",
    "                                        time_points=time_points, inputs=inputs)\n",
    "rate_model.time_points = time_points\n",
    "res = rate_model.simulate()\n",
    "\n",
    "f, ax = plt.subplots(figsize=(10,2*len(res.keys())), nrows=len(res.keys()), sharex=True, layout='constrained')\n",
    "# for k, (n, node) in enumerate(nodelist):\n",
    "for j,k in enumerate(res.keys()):\n",
    "    utils.simpleaxis(ax[j])\n",
    "    if hasattr(nn_chem_sub.neurons[str(k.name)], 'amplitude'):\n",
    "        ax[j].plot(np.arange(num_timepoints), np.array(nn_chem_sub.neurons[str(k.name)].amplitude), color='gray')\n",
    "        ax[j].set_title(f'{np.corrcoef(np.array(nn_chem_sub.neurons[str(k.name)].amplitude)[np.arange(num_timepoints)], res[k])[0,1]}')\n",
    "    # ax[j].plot(time_points, np.array(nn_chem_sub.neurons[str(k.name)].amplitude)[time_points], label=f'{k.name}-{nn_chem_sub.neurons[str(k.name)].name}', color='gray')\n",
    "    ax1 = ax[j]\n",
    "    ax1.plot(np.arange(num_timepoints), res[k], color='orange', label=f'{k.name}-{nn_chem_sub.neurons[str(k.name)].name}')\n",
    "    ax1.legend(frameon=False)\n",
    "f.suptitle(f'{database}')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "root_path = '/Users/sahilmoza/Documents/Codes/CEDNe/tmp/100_50_15/data_sources/downloads/Atanas_2023/Control'\n",
    "for study_name in os.listdir(f\"{root_path}\"):\n",
    "    if not study_name.startswith('.'):\n",
    "        print(study_name, root_path)\n",
    "\n",
    "        DB_PATH = f'sqlite:///{root_path}/{study_name}/cedne_optimization_optuna.db'\n",
    "        study_names = optuna.study.get_all_study_names(storage=DB_PATH)\n",
    "        print(\"Available studies:\", study_names)\n",
    "        print(study_name, DB_PATH)\n",
    "        study = optuna.load_study(study_name=f'../../data_sources/downloads/Atanas_2023/Control/{study_name}', storage=DB_PATH)\n",
    "        # Extract trial data\n",
    "        trials = study.trials\n",
    "        trial_numbers = [t.number for t in trials if t.state == optuna.trial.TrialState.COMPLETE]\n",
    "        loss_values = [t.value for t in trials if t.state == optuna.trial.TrialState.COMPLETE]\n",
    "        \n",
    "        # Plot loss over trials\n",
    "        f, ax = plt.subplots(figsize=(5,3))\n",
    "        ax.plot(trial_numbers, loss_values, marker=\"o\", linestyle=\"-\", color=\"gray\")\n",
    "        ax.set_yscale(\"log\")\n",
    "        ax.set_xlabel(\"Trial Number\")\n",
    "        ax.set_ylabel(\"Log Loss\")\n",
    "        ax.set_title(f\"{study_name} Loss over Trials\")\n",
    "        utils.simpleaxis(ax)\n",
    "        plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_params = {}\n",
    "root_path = '/Users/sahilmoza/Documents/Codes/CEDNe/tmp/100_50_15/data_sources/downloads/Atanas_2023/Control'\n",
    "for study_name in os.listdir(f\"{root_path}\"):\n",
    "    if not study_name.startswith('.'):\n",
    "        DB_PATH = f'sqlite:///{root_path}/{study_name}/cedne_optimization_optuna.db'\n",
    "        # Load study from database\n",
    "        print(study_name, DB_PATH)\n",
    "        study = optuna.load_study(study_name=f'../../data_sources/downloads/Atanas_2023/Control/{study_name}', storage=DB_PATH)\n",
    "        # Extract trial data\n",
    "        best_params[study_name] = study.best_params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_keys = []\n",
    "for study in best_params:\n",
    "    all_keys.append(best_params[study].keys())\n",
    "all_keys = set(all_keys[0]).union(*all_keys)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gain_list = {}\n",
    "tconst_list = {}\n",
    "base_list = {}\n",
    "weight_list = {}\n",
    "for j,key in enumerate(all_keys):\n",
    "    pref = key.split(\":\")[0]\n",
    "    if pref == 'gain':\n",
    "        suff = key.split(\":\")[1]\n",
    "        paramlist = []\n",
    "        for study in best_params:\n",
    "            if key in best_params[study]:\n",
    "                paramlist.append(best_params[study][key])\n",
    "        gain_list[suff] = paramlist\n",
    "    elif pref == 'time_constant':\n",
    "        suff = key.split(\":\")[1]\n",
    "        paramlist = []\n",
    "        for study in best_params:\n",
    "            if key in best_params[study]:\n",
    "                paramlist.append(best_params[study][key])\n",
    "        tconst_list[suff] = paramlist\n",
    "    elif pref == 'baseline':\n",
    "        suff = key.split(\":\")[1]\n",
    "        paramlist = []\n",
    "        for study in best_params:\n",
    "            if key in best_params[study]:\n",
    "                paramlist.append(best_params[study][key])\n",
    "        base_list[suff] = paramlist\n",
    "    elif pref == 'weight':\n",
    "        suff = '->'.join(key.split(\":\")[1:-1])\n",
    "        paramlist = []\n",
    "        for study in best_params:\n",
    "            if key in best_params[study]:\n",
    "                paramlist.append(best_params[study][key])\n",
    "        weight_list[suff] = paramlist\n",
    "    else:\n",
    "        print(f\"Unknown parameter: {key}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###############################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Path to your Optuna SQLite database\n",
    "CEDNE_ROOT = os.path.dirname(os.path.abspath(cedne.__file__))\n",
    "PACKAGE_ROOT = CEDNE_ROOT.split('src')[0]\n",
    "\n",
    "for study_name in os.listdir(f\"{PACKAGE_ROOT}/tmp/100_50_15/\"):\n",
    "    if study_name.startswith(\"Atanas\"):        \n",
    "        DB_PATH = f\"sqlite:///{PACKAGE_ROOT}/tmp/100_50_15/{study_name}/cedne_optimization_optuna.db\"  # Replace with your database path\n",
    "         # Load study from database\n",
    "        print(study_name, DB_PATH)\n",
    "        study = optuna.load_study(study_name=study_name, storage=DB_PATH)\n",
    "        # Extract trial data\n",
    "        trials = study.trials\n",
    "        trial_numbers = [t.number for t in trials if t.state == optuna.trial.TrialState.COMPLETE]\n",
    "        loss_values = [t.value for t in trials if t.state == optuna.trial.TrialState.COMPLETE]\n",
    "\n",
    "        # Plot loss over trials\n",
    "        f, ax = plt.subplots(figsize=(2,2))\n",
    "        ax.plot(trial_numbers, loss_values, marker=\"o\", linestyle=\"-\", color=\"gray\")\n",
    "        ax.set_yscale(\"log\")\n",
    "        ax.set_xlabel(\"Trial Number\", fontsize='xx-large')\n",
    "        ax.set_ylabel(\"Log Loss\", fontsize='xx-large')\n",
    "        ax.set_title(f\"{study_name} Loss over Trials\", fontsize='xx-large')\n",
    "        utils.simpleaxis(ax)\n",
    "        plt.savefig(f\"Loss_{study_name}.svg\", transparent=True)\n",
    "        plt.show()\n",
    "        plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_params = {}\n",
    "for study_name in os.listdir(f\"{PACKAGE_ROOT}/tmp/100_50_15\"):\n",
    "    if study_name.startswith(\"Atanas\"):        \n",
    "        DB_PATH = f\"sqlite:///{PACKAGE_ROOT}/tmp/100_50_15/{study_name}/cedne_optimization_optuna.db\"  # Replace with your database path\n",
    "         # Load study from database\n",
    "        print(study_name, DB_PATH)\n",
    "        study = optuna.load_study(study_name=study_name, storage=DB_PATH)\n",
    "        # Extract trial data\n",
    "        best_params[study_name] = study.best_params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_keys = []\n",
    "for study in best_params:\n",
    "    all_keys.append(best_params[study].keys())\n",
    "\n",
    "common_keys = set(all_keys[0]).intersection(*all_keys)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gain_list = {}\n",
    "tconst_list = {}\n",
    "base_list = {}\n",
    "weight_list = {}\n",
    "for j,key in enumerate(common_keys):\n",
    "    pref = key.split(\":\")[0]\n",
    "    if pref == 'gain':\n",
    "        suff = key.split(\":\")[1]\n",
    "        paramlist = []\n",
    "        for study in best_params:\n",
    "            paramlist.append(best_params[study][key])\n",
    "        gain_list[suff] = paramlist\n",
    "    elif pref == 'time_constant':\n",
    "        suff = key.split(\":\")[1]\n",
    "        paramlist = []\n",
    "        for study in best_params:\n",
    "            paramlist.append(best_params[study][key])\n",
    "        tconst_list[suff] = paramlist\n",
    "    elif pref == 'baseline':\n",
    "        suff = key.split(\":\")[1]\n",
    "        paramlist = []\n",
    "        for study in best_params:\n",
    "            paramlist.append(best_params[study][key])\n",
    "        base_list[suff] = paramlist\n",
    "    elif pref == 'weight':\n",
    "        suff = '->'.join(key.split(\":\")[1:-1])\n",
    "        paramlist = []\n",
    "        for study in best_params:\n",
    "            paramlist.append(best_params[study][key])\n",
    "        weight_list[suff] = paramlist\n",
    "    else:\n",
    "        print(f\"Unknown parameter: {key}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### Plotting the results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f, ax = plt.subplots(1, 1, figsize=(60, 3), layout='constrained')\n",
    "for j,key in enumerate(sorted(gain_list.keys())):\n",
    "    ax.scatter([j]*len(gain_list[key]), gain_list[key], color='gray', alpha=0.2)\n",
    "    ax.errorbar([j], y= np.mean(gain_list[key]), yerr=np.std(gain_list[key]), color='k', alpha=1, fmt='o')\n",
    "ax.set_xticks(range(len(gain_list)))\n",
    "ax.set_xticklabels(sorted(gain_list.keys()), rotation=45, fontsize='xx-large', ha='right')\n",
    "ax.tick_params(axis='y', labelsize='xx-large')\n",
    "utils.simpleaxis(ax)\n",
    "f.suptitle('Gain', fontsize='xx-large')\n",
    "plt.show()\n",
    "\n",
    "f, ax = plt.subplots(1, 1, figsize=(60, 3), layout='constrained')\n",
    "for j,key in enumerate(sorted(tconst_list.keys())):\n",
    "    ax.scatter([j]*len(tconst_list[key]), tconst_list[key], color='gray', alpha=0.2)\n",
    "    ax.errorbar([j], y= np.mean(tconst_list[key]), yerr=np.std(tconst_list[key]), color='k', alpha=1, fmt='o')\n",
    "ax.set_xticks(range(len(tconst_list)))\n",
    "ax.set_xticklabels(sorted(tconst_list.keys()), rotation=45, fontsize='xx-large', ha='right')\n",
    "ax.tick_params(axis='y', labelsize='xx-large')\n",
    "utils.simpleaxis(ax)\n",
    "f.suptitle('Time Constant', fontsize='xx-large')\n",
    "plt.show()\n",
    "\n",
    "f, ax = plt.subplots(1, 1, figsize=(60, 3), layout='constrained')\n",
    "for j,key in enumerate(sorted(base_list.keys())):\n",
    "    ax.scatter([j]*len(base_list[key]), base_list[key], color='gray', alpha=0.2)\n",
    "    ax.errorbar([j], y= np.mean(base_list[key]), yerr=np.std(base_list[key]), color='k', alpha=1, fmt='o')\n",
    "ax.set_xticks(range(len(base_list)))\n",
    "ax.set_xticklabels(sorted(base_list.keys()), rotation=45, fontsize='xx-large', ha='right')\n",
    "ax.tick_params(axis='y', labelsize='xx-large')\n",
    "utils.simpleaxis(ax)\n",
    "f.suptitle('Baseline', fontsize='xx-large')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "thres_val = 1\n",
    "rowwise = {}\n",
    "nrows = len(weight_list.keys())//100+1\n",
    "f, ax = plt.subplots(nrows, 1, figsize=(60, 3*nrows), layout='constrained')\n",
    "for j,key in enumerate(sorted(weight_list.keys())):\n",
    "    if not j//100 in rowwise:\n",
    "        rowwise[j//100] = []\n",
    "    ax[j//100].scatter([j%100]*len(weight_list[key]), weight_list[key], color='gray', alpha=0.2)\n",
    "    if np.mean(weight_list[key])>thres_val:\n",
    "        ax[j//100].errorbar([j%100], y= np.mean(weight_list[key]), yerr=np.std(weight_list[key]), color='orange', alpha=1, fmt='o')\n",
    "    elif np.mean(weight_list[key])<-thres_val:\n",
    "        ax[j//100].errorbar([j%100], y= np.mean(weight_list[key]), yerr=np.std(weight_list[key]), color='purple', alpha=1, fmt='o')\n",
    "    else:\n",
    "        ax[j//100].errorbar([j%100], y= np.mean(weight_list[key]), yerr=np.std(weight_list[key]), color='k', alpha=1, fmt='o')\n",
    "    rowwise[j//100].append(key)\n",
    "\n",
    "for key in rowwise:\n",
    "    ax[key].axhline(0, color='gray', linestyle='--', alpha=0.5)\n",
    "    ax[key].set_xticks(range(len(rowwise[key])))\n",
    "    ax[key].set_xticklabels(rowwise[key], rotation=45, fontsize='xx-large', ha='right')\n",
    "    # ax[key].set_yticklabels(ax[key].get_yticks(), fontsize='xx-large')\n",
    "    utils.simpleaxis(ax[key])\n",
    "    ax[key].tick_params(axis='y', labelsize='xx-large')\n",
    "f.suptitle('Weights')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "thres_val = 1\n",
    "rowwise = {}\n",
    "nrows = len(weight_list.keys())//100+1\n",
    "f, ax = plt.subplots(nrows, 1, figsize=(60, 3*nrows), layout='constrained')\n",
    "for j,(key,val) in enumerate(sorted(weight_list.items(),key= lambda x: x[0].split('->')[1])):\n",
    "    if not j//100 in rowwise:\n",
    "        rowwise[j//100] = []\n",
    "    ax[j//100].scatter([j%100]*len(weight_list[key]), weight_list[key], color='gray', alpha=0.2)\n",
    "    if np.mean(weight_list[key])>thres_val:\n",
    "        ax[j//100].errorbar([j%100], y= np.mean(weight_list[key]), yerr=np.std(weight_list[key]), color='orange', alpha=1, fmt='o')\n",
    "    elif np.mean(weight_list[key])<-thres_val:\n",
    "        ax[j//100].errorbar([j%100], y= np.mean(weight_list[key]), yerr=np.std(weight_list[key]), color='purple', alpha=1, fmt='o')\n",
    "    else:\n",
    "        ax[j//100].errorbar([j%100], y= np.mean(weight_list[key]), yerr=np.std(weight_list[key]), color='k', alpha=1, fmt='o')\n",
    "    rowwise[j//100].append(key)\n",
    "\n",
    "for key in rowwise:\n",
    "    ax[key].axhline(0, color='gray', linestyle='--', alpha=0.5)\n",
    "    ax[key].set_xticks(range(len(rowwise[key])))\n",
    "    ax[key].set_xticklabels(rowwise[key], rotation=45, fontsize='xx-large', ha='right')\n",
    "    # ax[key].set_yticklabels(ax[key].get_yticks(), fontsize='xx-large')\n",
    "    utils.simpleaxis(ax[key])\n",
    "    ax[key].tick_params(axis='y', labelsize='xx-large')\n",
    "f.suptitle('Weights')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Simulating the best parameter models. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "neurons = {}\n",
    "for study_name in best_params.keys():\n",
    "    sn = study_name.split('Atanas et al (2023) ')[1]\n",
    "    nrs = []\n",
    "    params = best_params[study_name]\n",
    "    for key in params.keys():\n",
    "        if key.startswith('weight'):\n",
    "            _, n1, n2, _ = key.split(':')\n",
    "            nrs.append(n1)\n",
    "            nrs.append(n2)\n",
    "            nrs = list(set(nrs))\n",
    "    neurons[sn] = nrs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "jsons = {}\n",
    "for js in os.listdir('/Users/sahilmoza/Documents/Postdoc/Yun Zhang/data/SteveFlavell-NeuroPAL-Cell/Control/'):\n",
    "    with open (\"/Users/sahilmoza/Documents/Postdoc/Yun Zhang/data/SteveFlavell-NeuroPAL-Cell/Control/{}\".format(js), 'r') as f:\n",
    "        jsons[js+'_100_50_15'] = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w = utils.makeWorm(chem_only=True)\n",
    "nn_chem = w.networks[\"Neutral\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "measuredNeurons = {}\n",
    "optim_neurs = {js:[] for js in jsons.keys()}\n",
    "for js, p in jsons.items():\n",
    "    sortedKeys = sorted ([int(x) for x in (p['labeled'].keys())])\n",
    "    labelledNeurons = {p['labeled'][str(x)]['label']:x for x in sortedKeys if not '?' in p['labeled'][str(x)]['label']} # Removing unsure hits\n",
    "    measuredNeurons[js] = {m:i for i,m in enumerate(set(labelledNeurons))}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for datab_in, database in enumerate(jsons.keys()):\n",
    "    nn_chem_sub = nn_chem.subnetwork(neuron_names=neurons[database])\n",
    "    for neuron in nn_chem_sub.neurons:\n",
    "        if neuron in measuredNeurons[database].keys():\n",
    "            nn_chem_sub.neurons[neuron].set_property('amplitude', jsons[database]['trace_array'][measuredNeurons[database][neuron]])\n",
    "\n",
    "    num_timepoints = len(jsons[database]['trace_array'][measuredNeurons[database][list(measuredNeurons[database].keys())[0]]])\n",
    "    input_nodes = [nn_chem_sub.neurons[n] for n in nn_chem_sub.neurons if nn_chem_sub.neurons[n].type == 'sensory']\n",
    "    inputs = []\n",
    "    time_points = np.arange(num_timepoints)\n",
    "    for inp in input_nodes:\n",
    "        if hasattr(inp, 'amplitude'):\n",
    "            input_value = {t:inp.amplitude[j] for j,t in enumerate(time_points)}\n",
    "            inputs.append(simulator.TimeSeriesInput([inp], input_value))\n",
    "    \n",
    "    study_name = 'Atanas et al (2023) ' + database\n",
    "    pre_params = best_params[study_name]\n",
    "    params,nrs = parse_parameters(pre_params)\n",
    "\n",
    "    baseline = {nn_chem_sub.neurons[n]:0 for n in nn_chem_sub.neurons}\n",
    "    gains = {nn_chem_sub.neurons[n]:1 for n in nn_chem_sub.neurons}\n",
    "    time_constants = {nn_chem_sub.neurons[n]:1 for n in nn_chem_sub.neurons}\n",
    "    weights = {(nn_chem_sub.neurons[e[0].name], nn_chem_sub.neurons[e[1].name]):1 for e in nn_chem_sub.edges}\n",
    "\n",
    "    baseline.update({nn_chem_sub.neurons[n]:v for n,v in params['baseline'].items()})\n",
    "    gains.update({nn_chem_sub.neurons[n]:v for n,v in params['gain'].items()})\n",
    "    time_constants.update({nn_chem_sub.neurons[n]:v for n,v in params['time_constant'].items()})\n",
    "    weights.update({(nn_chem_sub.neurons[e[0]],nn_chem_sub.neurons[e[1]]):v for e,v in params['weight'].items()})\n",
    "    \n",
    "    rate_model = simulator.RateModel(nn_chem_sub, input_nodes, weights, gains, time_constants, baseline, static_neurons=input_nodes, \\\n",
    "                                            time_points=time_points, inputs=inputs)\n",
    "    rate_model.time_points = time_points\n",
    "    res = rate_model.simulate()\n",
    "\n",
    "    f, ax = plt.subplots(figsize=(10,2*len(res.keys())), nrows=len(res.keys()), sharex=True, layout='constrained')\n",
    "    # for k, (n, node) in enumerate(nodelist):\n",
    "    for j,k in enumerate(res.keys()):\n",
    "        utils.simpleaxis(ax[j])\n",
    "        if hasattr(nn_chem_sub.neurons[str(k.name)], 'amplitude'):\n",
    "            ax[j].plot(np.arange(num_timepoints), np.array(nn_chem_sub.neurons[str(k.name)].amplitude), color='gray')\n",
    "            ax[j].set_title(f'{np.corrcoef(np.array(nn_chem_sub.neurons[str(k.name)].amplitude)[np.arange(num_timepoints)], res[k])[0,1]}')\n",
    "        # ax[j].plot(time_points, np.array(nn_chem_sub.neurons[str(k.name)].amplitude)[time_points], label=f'{k.name}-{nn_chem_sub.neurons[str(k.name)].name}', color='gray')\n",
    "        ax1 = ax[j]\n",
    "        ax1.plot(np.arange(num_timepoints), res[k], color='orange', label=f'{k.name}-{nn_chem_sub.neurons[str(k.name)].name}')\n",
    "        ax1.legend(frameon=False)\n",
    "    f.suptitle(f'{database}')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for n in rate_model.neurons:\n",
    "    print(n.name, n.type, rate_model.neurons[n].baseline, rate_model.neurons[n].gain, rate_model.neurons[n].time_constant)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for edata in rate_model.edges(data=True, keys=True):\n",
    "    print(edata)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for e in nn_chem_sub.edges:\n",
    "    print(weights[e])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weight_list.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "utils.loadSynapticWeights(nn_chem)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "leifer_weight = {}\n",
    "for key in weight_list.keys():\n",
    "    n1,n2 = key.split('->')\n",
    "    c = nn_chem.connections[(nn_chem.neurons[n1], nn_chem.neurons[n2],0)]\n",
    "    leifer_weight[key] = c.weight\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fit_w = []\n",
    "leifer_w = []\n",
    "thres_val = 0.2\n",
    "for key in weight_list.keys():\n",
    "    if np.abs(leifer_weight[key])>thres_val and np.abs(np.mean(weight_list[key]))>thres_val:\n",
    "        leifer_w.append(leifer_weight[key])\n",
    "        fit_w.append(np.mean(weight_list[key]))\n",
    "fit_w , leifer_w = np.array(fit_w), np.array(leifer_w)\n",
    "f, ax = plt.subplots(figsize=(3,3))\n",
    "ax.scatter(np.abs(fit_w), np.abs(leifer_w), color='gray', alpha=1)\n",
    "slope, intercept, r_value, p_value, std_err = ss.linregress(np.abs(fit_w), np.abs(leifer_w))\n",
    "x = np.linspace(0,2,100)\n",
    "ax.plot(x, slope*x+intercept, color='k')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "flat_w = [item for sublist in weight_list.values() for item in sublist]\n",
    "flat_gain = [item for sublist in gain_list.values() for item in sublist]\n",
    "flat_tconst = [item for sublist in tconst_list.values() for item in sublist]\n",
    "flat_base = [item for sublist in base_list.values() for item in sublist]\n",
    "\n",
    "plt.hist(flat_w)\n",
    "plt.title('Weights')\n",
    "plt.show()\n",
    "\n",
    "plt.hist(flat_gain)\n",
    "plt.title('Gains')\n",
    "plt.show()\n",
    "\n",
    "plt.hist(flat_tconst)\n",
    "plt.title('Time Constants')\n",
    "plt.show()\n",
    "\n",
    "plt.hist(flat_base)\n",
    "plt.title('Baselines')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "3*len(gain_list) + len(weight_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "for study_name in os.listdir(f\"{PACKAGE_ROOT}/tmp\"):\n",
    "    if study_name.startswith(\"Atanas\"):        \n",
    "        DB_PATH = f\"sqlite:///{PACKAGE_ROOT}/tmp/{study_name}/cedne_optimization_optuna.db\"  # Replace with your database path\n",
    "         # Load study from database\n",
    "        print(study_name, DB_PATH)\n",
    "        study = optuna.load_study(study_name=study_name, storage=DB_PATH)\n",
    "        # Extract trial data\n",
    "        data_trials = np.array([list(trial.params.values()) for trial in study.trials if trial.value is not None])\n",
    "        pca = PCA().fit(data_trials)\n",
    "        explained_variance = np.cumsum(pca.explained_variance_ratio_)\n",
    "\n",
    "        # Plot the variance explained by each principal component\n",
    "        plt.plot(range(1, len(explained_variance)+1), explained_variance, marker='o')\n",
    "        plt.xlabel(\"Number of Principal Components\")\n",
    "        plt.ylabel(\"Cumulative Explained Variance\")\n",
    "        plt.title(\"Effective Dimensionality of Optimized Parameters\")\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_trials"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
